{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bloomberg_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQuoP_bf1mm5"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os \n",
        "import urllib\n",
        "import json\n",
        "import pandas as pd\n",
        "import tensorflow as tf "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HdYBMXO1Vid",
        "outputId": "93308da0-e3ba-467c-a10d-2f60550009e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTXCmC504tvd"
      },
      "source": [
        "def read(files):\n",
        "  for filename in files:\n",
        "    filename = '/content/gdrive/MyDrive/docs/' + filename      \n",
        "    with open(filename) as open_file:\n",
        "          content = open_file.read()\n",
        "          soup = BeautifulSoup(content, \"xml\")\n",
        "          print(soup.prettify())\n",
        "          print(\"First AGENCY tag in \" + filename[len(path) + 1:] + \": \" + str(soup.find('AGENCY')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec8bjZBD3ewJ"
      },
      "source": [
        "def extract_agency(infile):\n",
        "  contents = infile.read()\n",
        "  soup = BeautifulSoup(contents, 'xml')\n",
        "  agencies = soup.find_all('AGENCY')\n",
        "  for agency in agencies:\n",
        "    print(agency.get_text())\n",
        "\n",
        "\n",
        "def extract_time(infile):\n",
        "  contents = infile.read()\n",
        "  soup = BeautifulSoup(contents, 'xml')\n",
        "  time = soup.find('EFFDATE')\n",
        "  if time is not None:\n",
        "    time = time.text\n",
        "    start_idx = time.find(\"<P>\")\n",
        "    time = time[start_idx + 1:]\n",
        "    end_idx = time.find(\"<P>\")\n",
        "    time = time[0:end_idx]\n",
        "    time = time.split(\"\\n\")[2]\n",
        "  print(time)\n",
        "\n",
        "\n",
        "def extract_subject(infile):\n",
        "  contents = infile.read()\n",
        "  soup = BeautifulSoup(contents, 'xml')\n",
        "  subject = soup.find('SUBJECT')\n",
        "  print(subject)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQOKik97BV6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33665ee6-08cd-465f-fc78-d9921c1fc0bb"
      },
      "source": [
        "def get_json(url, tags):\n",
        "  values = []\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    json_file = response.read()\n",
        "    j = json.loads(json_file)\n",
        "    for i in tags:\n",
        "      values.append(j[i])\n",
        "    \n",
        "  return values\n",
        "\n",
        "# title and publication_date\n",
        "\n",
        "def populate_df(files, tags):\n",
        "  all_values = []\n",
        "  for i in files:\n",
        "    fname = i.replace(\".xml\", \".json\")\n",
        "    v = get_json(\"https://www.federalregister.gov/api/v1/documents/\" + fname, tags)\n",
        "    all_values.append(v)\n",
        "  \n",
        "  return all_values\n",
        "\n",
        "\n",
        "files = os.listdir('/content/gdrive/MyDrive/docs/')\n",
        "t = [\"title\", \"abstract\", \"publication_date\", \"topics\"]\n",
        "s = populate_df(files, t)\n",
        "d = {}\n",
        "d[\"filename\"] = files\n",
        "\n",
        "for i in range(0, len(t)):\n",
        "  col = [row[i] for row in s]\n",
        "  d[t[i]] = col\n",
        "\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "print(df)\n",
        "\n",
        "df.to_csv(\"fed_doc_data_abs.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            filename  ...                                             topics\n",
            "0     2014-21224.xml  ...  [Accounting, Administrative practice and proce...\n",
            "1     2020-08603.xml  ...  [Business and industry, Cooperatives, Reportin...\n",
            "2     2020-04776.xml  ...  [Reporting and recordkeeping requirements, Sec...\n",
            "3        02-5778.xml  ...  [Commodity futures, Reporting and recordkeepin...\n",
            "4     2020-25747.xml  ...            [Administrative practice and procedure]\n",
            "...              ...  ...                                                ...\n",
            "3613  2011-28313.xml  ...  [Administrative practice and procedure, Confli...\n",
            "3614   2012-6799.xml  ...                                                 []\n",
            "3615  2011-18777.xml  ...  [Advertising, Brokers, Commodity futures, Cons...\n",
            "3616  2011-11551.xml  ...  [Advertising, Brokers, Commodity futures, Cons...\n",
            "3617      01-175.xml  ...  [Administrative practice and procedure, Author...\n",
            "\n",
            "[3618 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ]
}